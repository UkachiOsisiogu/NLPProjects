{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my project defence I plan to analyse three different datasets with different algorithms - my plan now is to use the same algorithms and then classify with just the datasets at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes for IMDB Movie review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB dataset having 50K movie reviews for natural language processing or Text analytics. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms. For more dataset information, please go through the following link, http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   # a useful datastructure\n",
    "import pandas as pd  # for data preprocessing\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"IMDB_Dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    positive\n",
       "1    positive\n",
       "2    positive\n",
       "3    negative\n",
       "4    positive\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {'positive':1, 'negative' :0}\n",
    "\n",
    "def preprocess_y(sentiment):\n",
    "    return label[sentiment]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.apply(preprocess_y)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    One of the other reviewers has mentioned that ...\n",
       "1    A wonderful little production. <br /><br />The...\n",
       "2    I thought this was a wonderful way to spend ti...\n",
       "3    Basically there's a family where a little boy ...\n",
       "4    Petter Mattei's \"Love in the Time of Money\" is...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.review\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ukachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ukachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    One reviewers mentioned watching 1 Oz episode ...\n",
       "1    A wonderful little production . < br / > < br ...\n",
       "2    I thought wonderful way spend time hot summer ...\n",
       "3    Basically 's family little boy ( Jake ) thinks...\n",
       "4    Petter Mattei 's `` Love Time Money '' visuall...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def preprocess(review):\n",
    "    #convert the tweet to lower case\n",
    "    review.lower()\n",
    "    #convert all urls to sting \"URL\"\n",
    "    review = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',review)\n",
    "    #convert all @username to \"AT_USER\"\n",
    "    review = re.sub('@[^\\s]+','AT_USER', review)\n",
    "    #correct all multiple white spaces to a single white space\n",
    "    review = re.sub('[\\s]+', ' ', review)\n",
    "    #convert \"#topic\" to just \"topic\"\n",
    "    review = re.sub(r'#([^\\s]+)', r'\\1', review)\n",
    "    tokens = word_tokenize(review)\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "X = X.apply(preprocess)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction. text import TfidfVectorizer\n",
    "\n",
    "def feature_extraction(data):\n",
    "    tfv=TfidfVectorizer(sublinear_tf=True, stop_words=\"english\")\n",
    "    \n",
    "    features = tfv.fit_transform(data)\n",
    "    pickle.dump(tfv.vocabulary_,open(\"nb_feature.pkl\",\"wb\"))\n",
    "    return features\n",
    "\n",
    "data = np.array(X)\n",
    "label = np.array(y)\n",
    "\n",
    "features = feature_extraction(data)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 75175)\t0.07683718114148\n",
      "  (0, 57308)\t0.06624810814175679\n",
      "  (0, 97762)\t0.06602760412271698\n",
      "  (0, 65154)\t0.2509078981889345\n",
      "  (0, 29979)\t0.09795247296693152\n",
      "  (0, 52987)\t0.09506738058419874\n",
      "  (0, 42657)\t0.0840621839588345\n",
      "  (0, 75579)\t0.07345376450404376\n",
      "  (0, 30834)\t0.0584006628471939\n",
      "  (0, 40269)\t0.0581639744133865\n",
      "  (0, 11951)\t0.058076375863825386\n",
      "  (0, 89993)\t0.03916872881162975\n",
      "  (0, 86227)\t0.1423296497616121\n",
      "  (0, 12884)\t0.09343899459074295\n",
      "  (0, 94168)\t0.11301273835429296\n",
      "  (0, 78409)\t0.03850810122811244\n",
      "  (0, 96683)\t0.14130598712509415\n",
      "  (0, 79965)\t0.04717833901676\n",
      "  (0, 99600)\t0.10049007287848823\n",
      "  (0, 92537)\t0.07374873091476265\n",
      "  (0, 31776)\t0.0999572415413897\n",
      "  (0, 40989)\t0.07831441586249126\n",
      "  (0, 90563)\t0.10724872055245468\n",
      "  (0, 71260)\t0.08008369873381428\n",
      "  (0, 71307)\t0.09436521863985418\n",
      "  :\t:\n",
      "  (49999, 94206)\t0.11490080515017852\n",
      "  (49999, 51790)\t0.10313084807864072\n",
      "  (49999, 45842)\t0.20268533892717316\n",
      "  (49999, 31177)\t0.11845532639332741\n",
      "  (49999, 7096)\t0.12974722810563444\n",
      "  (49999, 40355)\t0.13857766525753998\n",
      "  (49999, 61445)\t0.12896296942002228\n",
      "  (49999, 44717)\t0.12147211376072924\n",
      "  (49999, 44394)\t0.18464662337023316\n",
      "  (49999, 32012)\t0.1158248862339517\n",
      "  (49999, 5981)\t0.12097096115480967\n",
      "  (49999, 92028)\t0.17405953427252366\n",
      "  (49999, 49608)\t0.199991782453494\n",
      "  (49999, 84428)\t0.2183820885335562\n",
      "  (49999, 56465)\t0.21582760387218292\n",
      "  (49999, 74379)\t0.16883093252217782\n",
      "  (49999, 29980)\t0.1345418200090843\n",
      "  (49999, 13777)\t0.15932909479586663\n",
      "  (49999, 15868)\t0.18954538768738746\n",
      "  (49999, 31189)\t0.1995282599610079\n",
      "  (49999, 37976)\t0.17128281597429448\n",
      "  (49999, 74372)\t0.16421383401792986\n",
      "  (49999, 60123)\t0.19316732872744183\n",
      "  (49999, 21180)\t0.21975063958521368\n",
      "  (49999, 100572)\t0.27765865992871525\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8652818462053554\n",
      "Recall: 0.8651850607402429\n",
      "Fscore: 0.8651885895622204\n",
      "Support: None\n"
     ]
    }
   ],
   "source": [
    "# Fitting Multinomial Naive Bayes classifier to the Training set\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "precision, recall, fscore, support = score(y_test, y_pred,average='macro')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('Fscore: {}'.format(fscore))\n",
    "print('Support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.850153619775179\n",
      "Recall: 0.8482271929087717\n",
      "Fscore: 0.8480794706403763\n",
      "Support: None\n"
     ]
    }
   ],
   "source": [
    "# Fitting Bernoulli Naive Bayes classifier to the Training set\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier = BernoulliNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "precision, recall, fscore, support = score(y_test, y_pred,average='macro')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('Fscore: {}'.format(fscore))\n",
    "print('Support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 75175)\t0.07683718114148\n",
      "  (0, 57308)\t0.06624810814175679\n",
      "  (0, 97762)\t0.06602760412271698\n",
      "  (0, 65154)\t0.2509078981889345\n",
      "  (0, 29979)\t0.09795247296693152\n",
      "  (0, 52987)\t0.09506738058419874\n",
      "  (0, 42657)\t0.0840621839588345\n",
      "  (0, 75579)\t0.07345376450404376\n",
      "  (0, 30834)\t0.0584006628471939\n",
      "  (0, 40269)\t0.0581639744133865\n",
      "  (0, 11951)\t0.058076375863825386\n",
      "  (0, 89993)\t0.03916872881162975\n",
      "  (0, 86227)\t0.1423296497616121\n",
      "  (0, 12884)\t0.09343899459074295\n",
      "  (0, 94168)\t0.11301273835429296\n",
      "  (0, 78409)\t0.03850810122811244\n",
      "  (0, 96683)\t0.14130598712509415\n",
      "  (0, 79965)\t0.04717833901676\n",
      "  (0, 99600)\t0.10049007287848823\n",
      "  (0, 92537)\t0.07374873091476265\n",
      "  (0, 31776)\t0.0999572415413897\n",
      "  (0, 40989)\t0.07831441586249126\n",
      "  (0, 90563)\t0.10724872055245468\n",
      "  (0, 71260)\t0.08008369873381428\n",
      "  (0, 71307)\t0.09436521863985418\n",
      "  :\t:\n",
      "  (49999, 94206)\t0.11490080515017852\n",
      "  (49999, 51790)\t0.10313084807864072\n",
      "  (49999, 45842)\t0.20268533892717316\n",
      "  (49999, 31177)\t0.11845532639332741\n",
      "  (49999, 7096)\t0.12974722810563444\n",
      "  (49999, 40355)\t0.13857766525753998\n",
      "  (49999, 61445)\t0.12896296942002228\n",
      "  (49999, 44717)\t0.12147211376072924\n",
      "  (49999, 44394)\t0.18464662337023316\n",
      "  (49999, 32012)\t0.1158248862339517\n",
      "  (49999, 5981)\t0.12097096115480967\n",
      "  (49999, 92028)\t0.17405953427252366\n",
      "  (49999, 49608)\t0.199991782453494\n",
      "  (49999, 84428)\t0.2183820885335562\n",
      "  (49999, 56465)\t0.21582760387218292\n",
      "  (49999, 74379)\t0.16883093252217782\n",
      "  (49999, 29980)\t0.1345418200090843\n",
      "  (49999, 13777)\t0.15932909479586663\n",
      "  (49999, 15868)\t0.18954538768738746\n",
      "  (49999, 31189)\t0.1995282599610079\n",
      "  (49999, 37976)\t0.17128281597429448\n",
      "  (49999, 74372)\t0.16421383401792986\n",
      "  (49999, 60123)\t0.19316732872744183\n",
      "  (49999, 21180)\t0.21975063958521368\n",
      "  (49999, 100572)\t0.27765865992871525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def feature_extraction(data):\n",
    "    tfv=TfidfVectorizer(sublinear_tf=True, stop_words = \"english\")\n",
    "    features=tfv.fit_transform(data)\n",
    "    pickle.dump(tfv.vocabulary_, open(\"svm_feature.pkl\", \"wb\"))\n",
    "    return features\n",
    "\n",
    "data = np.array(X)\n",
    "label = np.array(y)\n",
    "features = feature_extraction(data)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.896580022023722\n",
      "Recall: 0.8965145860583442\n",
      "Fscore: 0.8964968690302881\n",
      "Support: None\n"
     ]
    }
   ],
   "source": [
    "# Fitting Support Vector Machine Classifier to the Training set\n",
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "precision, recall, fscore, support = score(y_test, y_pred,average='macro')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('Fscore: {}'.format(fscore))\n",
    "print('Support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8781164258749785\n",
      "Recall: 0.8781069124276497\n",
      "Fscore: 0.8780996477079819\n",
      "Support: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ukachi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fitting Logistic Regression Classifier to the Training set\n",
    "from sklearn import linear_model\n",
    "logReg = linear_model.LogisticRegression(solver='lbfgs', C=1000)\n",
    "logClassifier = logReg.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = logClassifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "precision, recall, fscore, support = score(y_test, y_pred,average='macro')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('Fscore: {}'.format(fscore))\n",
    "print('Support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fcb9eb20551a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcheck_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dot'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'-Tpng'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tree.dot'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'-o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tree.png'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[0mcheck_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ls\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-l\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \"\"\"\n\u001b[1;32m--> 336\u001b[1;33m     \u001b[0mretcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"args\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ls\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-l\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \"\"\"\n\u001b[1;32m--> 317\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    767\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    770\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1170\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1172\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1173\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "from subprocess import check_call\n",
    "check_call(['dot','-Tpng','tree.dot','-o','tree.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
